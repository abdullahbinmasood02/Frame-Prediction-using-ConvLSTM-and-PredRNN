{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Mount Google Drive to access it from Colab\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "wcumxh2PtCli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b0e8a6-8d48-46a2-f5aa-5b5d33d3d685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `unzip -q /content/val.zip -d /content/drive/MyDrive/dl_project_data/val (1)'\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `unzip -q /content/test.zip -d /content/drive/MyDrive/dl_project_data/test (1)'\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `unzip -q /content/train.zip -d /content/drive/MyDrive/dl_project_data/train (1)'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip    /content/val.zip -d /content/val\n",
        "!unzip    /content/test.zip -d /content/test\n",
        "!unzip  /content/train.zip -d /content/train\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy3I88zcWgjP",
        "outputId": "0fd94c73-45a2-4cf0-c1d9-733e35e06e99",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/val.zip\n",
            "   creating: /content/val/ApplyLipstick/\n",
            "  inflating: /content/val/ApplyLipstick/v_ApplyLipstick_g01_c05.avi  \n",
            "  inflating: /content/val/ApplyLipstick/v_ApplyLipstick_g07_c02.avi  \n",
            "  inflating: /content/val/ApplyLipstick/v_ApplyLipstick_g09_c04.avi  \n",
            "  inflating: /content/val/ApplyLipstick/v_ApplyLipstick_g10_c03.avi  \n",
            "  inflating: /content/val/ApplyLipstick/v_ApplyLipstick_g11_c03.avi  \n",
            "  inflating: /content/val/ApplyLipstick/v_ApplyLipstick_g13_c04.avi  \n",
            "  inflating: /content/val/ApplyLipstick/v_ApplyLipstick_g14_c01.avi  \n",
            "  inflating: /content/val/ApplyLipstick/v_ApplyLipstick_g15_c01.avi  \n",
            "  inflating: /content/val/ApplyLipstick/v_ApplyLipstick_g15_c02.avi  \n",
            "  inflating: /content/val/ApplyLipstick/v_ApplyLipstick_g16_c05.avi  \n",
            "  inflating: /content/val/ApplyLipstick/v_ApplyLipstick_g18_c03.avi  \n",
            "  inflating: /content/val/ApplyLipstick/v_ApplyLipstick_g19_c01.avi  \n",
            "  inflating: /content/val/ApplyLipstick/v_ApplyLipstick_g23_c01.avi  \n",
            "  inflating: /content/val/ApplyLipstick/v_ApplyLipstick_g24_c04.avi  \n",
            "   creating: /content/val/Archery/\n",
            "  inflating: /content/val/Archery/v_Archery_g01_c03.avi  \n",
            "  inflating: /content/val/Archery/v_Archery_g02_c03.avi  \n",
            "  inflating: /content/val/Archery/v_Archery_g06_c01.avi  \n",
            "  inflating: /content/val/Archery/v_Archery_g07_c02.avi  \n",
            "  inflating: /content/val/Archery/v_Archery_g09_c02.avi  \n",
            "  inflating: /content/val/Archery/v_Archery_g09_c03.avi  \n",
            "  inflating: /content/val/Archery/v_Archery_g10_c06.avi  \n",
            "  inflating: /content/val/Archery/v_Archery_g11_c07.avi  \n",
            "  inflating: /content/val/Archery/v_Archery_g13_c06.avi  \n",
            "  inflating: /content/val/Archery/v_Archery_g14_c04.avi  \n",
            "  inflating: /content/val/Archery/v_Archery_g15_c07.avi  \n",
            "  inflating: /content/val/Archery/v_Archery_g16_c04.avi  \n",
            "  inflating: /content/val/Archery/v_Archery_g18_c02.avi  \n",
            "  inflating: /content/val/Archery/v_Archery_g18_c04.avi  \n",
            "  inflating: /content/val/Archery/v_Archery_g21_c02.avi  \n",
            "  inflating: /content/val/Archery/v_Archery_g23_c04.avi  \n",
            "  inflating: /content/val/Archery/v_Archery_g25_c01.avi  \n",
            "  inflating: /content/val/Archery/v_Archery_g25_c02.avi  \n",
            "   creating: /content/val/BabyCrawling/\n",
            "  inflating: /content/val/BabyCrawling/v_BabyCrawling_g01_c01.avi  \n",
            "  inflating: /content/val/BabyCrawling/v_BabyCrawling_g03_c01.avi  \n",
            "  inflating: /content/val/BabyCrawling/v_BabyCrawling_g03_c03.avi  \n",
            "  inflating: /content/val/BabyCrawling/v_BabyCrawling_g07_c05.avi  \n",
            "  inflating: /content/val/BabyCrawling/v_BabyCrawling_g08_c01.avi  \n",
            "  inflating: /content/val/BabyCrawling/v_BabyCrawling_g08_c03.avi  \n",
            "  inflating: /content/val/BabyCrawling/v_BabyCrawling_g10_c03.avi  \n",
            "  inflating: /content/val/BabyCrawling/v_BabyCrawling_g12_c01.avi  \n",
            "  inflating: /content/val/BabyCrawling/v_BabyCrawling_g12_c04.avi  \n",
            "  inflating: /content/val/BabyCrawling/v_BabyCrawling_g13_c05.avi  \n",
            "  inflating: /content/val/BabyCrawling/v_BabyCrawling_g15_c02.avi  \n",
            "  inflating: /content/val/BabyCrawling/v_BabyCrawling_g19_c02.avi  \n",
            "  inflating: /content/val/BabyCrawling/v_BabyCrawling_g20_c03.avi  \n",
            "  inflating: /content/val/BabyCrawling/v_BabyCrawling_g20_c05.avi  \n",
            "  inflating: /content/val/BabyCrawling/v_BabyCrawling_g22_c03.avi  \n",
            "  inflating: /content/val/BabyCrawling/v_BabyCrawling_g24_c05.avi  \n",
            "   creating: /content/val/BalanceBeam/\n",
            "  inflating: /content/val/BalanceBeam/v_BalanceBeam_g02_c01.avi  \n",
            "  inflating: /content/val/BalanceBeam/v_BalanceBeam_g02_c04.avi  \n",
            "  inflating: /content/val/BalanceBeam/v_BalanceBeam_g04_c02.avi  \n",
            "  inflating: /content/val/BalanceBeam/v_BalanceBeam_g05_c03.avi  \n",
            "  inflating: /content/val/BalanceBeam/v_BalanceBeam_g06_c01.avi  \n",
            "  inflating: /content/val/BalanceBeam/v_BalanceBeam_g07_c01.avi  \n",
            "  inflating: /content/val/BalanceBeam/v_BalanceBeam_g08_c01.avi  \n",
            "  inflating: /content/val/BalanceBeam/v_BalanceBeam_g09_c03.avi  \n",
            "  inflating: /content/val/BalanceBeam/v_BalanceBeam_g13_c05.avi  \n",
            "  inflating: /content/val/BalanceBeam/v_BalanceBeam_g17_c01.avi  \n",
            "  inflating: /content/val/BalanceBeam/v_BalanceBeam_g19_c04.avi  \n",
            "  inflating: /content/val/BalanceBeam/v_BalanceBeam_g21_c02.avi  \n",
            "  inflating: /content/val/BalanceBeam/v_BalanceBeam_g22_c03.avi  \n",
            "   creating: /content/val/ApplyEyeMakeup/\n",
            "  inflating: /content/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c05.avi  \n",
            "  inflating: /content/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c01.avi  \n",
            "  inflating: /content/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c07.avi  \n",
            "  inflating: /content/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g06_c07.avi  \n",
            "  inflating: /content/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi  \n",
            "  inflating: /content/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi  \n",
            "  inflating: /content/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi  \n",
            "  inflating: /content/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi  \n",
            "  inflating: /content/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g11_c02.avi  \n",
            "  inflating: /content/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g12_c05.avi  \n",
            "  inflating: /content/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c01.avi  \n",
            "  inflating: /content/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c07.avi  \n",
            "  inflating: /content/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g16_c04.avi  \n",
            "  inflating: /content/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g18_c02.avi  \n",
            "  inflating: /content/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g20_c04.avi  \n",
            "  inflating: /content/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g22_c01.avi  \n",
            "  inflating: /content/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g24_c06.avi  \n",
            "  inflating: /content/val/ApplyEyeMakeup/v_ApplyEyeMakeup_g25_c07.avi  \n",
            "Archive:  /content/test.zip\n",
            "   creating: /content/test/ApplyLipstick/\n",
            "  inflating: /content/test/ApplyLipstick/v_ApplyLipstick_g01_c04.avi  \n",
            "  inflating: /content/test/ApplyLipstick/v_ApplyLipstick_g03_c01.avi  \n",
            "  inflating: /content/test/ApplyLipstick/v_ApplyLipstick_g04_c02.avi  \n",
            "  inflating: /content/test/ApplyLipstick/v_ApplyLipstick_g06_c05.avi  \n",
            "  inflating: /content/test/ApplyLipstick/v_ApplyLipstick_g12_c03.avi  \n",
            "  inflating: /content/test/ApplyLipstick/v_ApplyLipstick_g12_c04.avi  \n",
            "  inflating: /content/test/ApplyLipstick/v_ApplyLipstick_g14_c03.avi  \n",
            "  inflating: /content/test/ApplyLipstick/v_ApplyLipstick_g15_c04.avi  \n",
            "  inflating: /content/test/ApplyLipstick/v_ApplyLipstick_g17_c05.avi  \n",
            "  inflating: /content/test/ApplyLipstick/v_ApplyLipstick_g20_c02.avi  \n",
            "  inflating: /content/test/ApplyLipstick/v_ApplyLipstick_g20_c03.avi  \n",
            "  inflating: /content/test/ApplyLipstick/v_ApplyLipstick_g20_c04.avi  \n",
            "  inflating: /content/test/ApplyLipstick/v_ApplyLipstick_g21_c02.avi  \n",
            "  inflating: /content/test/ApplyLipstick/v_ApplyLipstick_g22_c01.avi  \n",
            "  inflating: /content/test/ApplyLipstick/v_ApplyLipstick_g23_c03.avi  \n",
            "   creating: /content/test/Archery/\n",
            "  inflating: /content/test/Archery/v_Archery_g02_c01.avi  \n",
            "  inflating: /content/test/Archery/v_Archery_g07_c05.avi  \n",
            "  inflating: /content/test/Archery/v_Archery_g08_c01.avi  \n",
            "  inflating: /content/test/Archery/v_Archery_g08_c03.avi  \n",
            "  inflating: /content/test/Archery/v_Archery_g10_c02.avi  \n",
            "  inflating: /content/test/Archery/v_Archery_g11_c01.avi  \n",
            "  inflating: /content/test/Archery/v_Archery_g11_c03.avi  \n",
            "  inflating: /content/test/Archery/v_Archery_g11_c05.avi  \n",
            "  inflating: /content/test/Archery/v_Archery_g12_c04.avi  \n",
            "  inflating: /content/test/Archery/v_Archery_g15_c04.avi  \n",
            "  inflating: /content/test/Archery/v_Archery_g16_c05.avi  \n",
            "  inflating: /content/test/Archery/v_Archery_g17_c04.avi  \n",
            "  inflating: /content/test/Archery/v_Archery_g18_c05.avi  \n",
            "  inflating: /content/test/Archery/v_Archery_g19_c04.avi  \n",
            "  inflating: /content/test/Archery/v_Archery_g21_c03.avi  \n",
            "  inflating: /content/test/Archery/v_Archery_g22_c05.avi  \n",
            "  inflating: /content/test/Archery/v_Archery_g23_c01.avi  \n",
            "  inflating: /content/test/Archery/v_Archery_g24_c06.avi  \n",
            "  inflating: /content/test/Archery/v_Archery_g25_c06.avi  \n",
            "   creating: /content/test/BabyCrawling/\n",
            "  inflating: /content/test/BabyCrawling/v_BabyCrawling_g01_c04.avi  \n",
            "  inflating: /content/test/BabyCrawling/v_BabyCrawling_g02_c06.avi  \n",
            "  inflating: /content/test/BabyCrawling/v_BabyCrawling_g09_c03.avi  \n",
            "  inflating: /content/test/BabyCrawling/v_BabyCrawling_g10_c04.avi  \n",
            "  inflating: /content/test/BabyCrawling/v_BabyCrawling_g13_c03.avi  \n",
            "  inflating: /content/test/BabyCrawling/v_BabyCrawling_g13_c06.avi  \n",
            "  inflating: /content/test/BabyCrawling/v_BabyCrawling_g15_c06.avi  \n",
            "  inflating: /content/test/BabyCrawling/v_BabyCrawling_g16_c01.avi  \n",
            "  inflating: /content/test/BabyCrawling/v_BabyCrawling_g17_c01.avi  \n",
            "  inflating: /content/test/BabyCrawling/v_BabyCrawling_g17_c05.avi  \n",
            "  inflating: /content/test/BabyCrawling/v_BabyCrawling_g18_c01.avi  \n",
            "  inflating: /content/test/BabyCrawling/v_BabyCrawling_g20_c01.avi  \n",
            "  inflating: /content/test/BabyCrawling/v_BabyCrawling_g20_c07.avi  \n",
            "  inflating: /content/test/BabyCrawling/v_BabyCrawling_g22_c06.avi  \n",
            "  inflating: /content/test/BabyCrawling/v_BabyCrawling_g24_c06.avi  \n",
            "  inflating: /content/test/BabyCrawling/v_BabyCrawling_g25_c01.avi  \n",
            "  inflating: /content/test/BabyCrawling/v_BabyCrawling_g25_c03.avi  \n",
            "   creating: /content/test/BalanceBeam/\n",
            "  inflating: /content/test/BalanceBeam/v_BalanceBeam_g05_c04.avi  \n",
            "  inflating: /content/test/BalanceBeam/v_BalanceBeam_g06_c02.avi  \n",
            "  inflating: /content/test/BalanceBeam/v_BalanceBeam_g07_c04.avi  \n",
            "  inflating: /content/test/BalanceBeam/v_BalanceBeam_g09_c04.avi  \n",
            "  inflating: /content/test/BalanceBeam/v_BalanceBeam_g10_c02.avi  \n",
            "  inflating: /content/test/BalanceBeam/v_BalanceBeam_g10_c04.avi  \n",
            "  inflating: /content/test/BalanceBeam/v_BalanceBeam_g11_c02.avi  \n",
            "  inflating: /content/test/BalanceBeam/v_BalanceBeam_g11_c04.avi  \n",
            "  inflating: /content/test/BalanceBeam/v_BalanceBeam_g15_c03.avi  \n",
            "  inflating: /content/test/BalanceBeam/v_BalanceBeam_g20_c01.avi  \n",
            "  inflating: /content/test/BalanceBeam/v_BalanceBeam_g21_c03.avi  \n",
            "  inflating: /content/test/BalanceBeam/v_BalanceBeam_g23_c03.avi  \n",
            "  inflating: /content/test/BalanceBeam/v_BalanceBeam_g24_c03.avi  \n",
            "  inflating: /content/test/BalanceBeam/v_BalanceBeam_g25_c03.avi  \n",
            "   creating: /content/test/ApplyEyeMakeup/\n",
            "  inflating: /content/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi  \n",
            "  inflating: /content/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi  \n",
            "  inflating: /content/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g02_c01.avi  \n",
            "  inflating: /content/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g02_c04.avi  \n",
            "  inflating: /content/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g04_c07.avi  \n",
            "  inflating: /content/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c03.avi  \n",
            "  inflating: /content/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c05.avi  \n",
            "  inflating: /content/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g06_c02.avi  \n",
            "  inflating: /content/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c01.avi  \n",
            "  inflating: /content/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g10_c01.avi  \n",
            "  inflating: /content/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g14_c02.avi  \n",
            "  inflating: /content/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g14_c05.avi  \n",
            "  inflating: /content/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g16_c03.avi  \n",
            "  inflating: /content/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g17_c03.avi  \n",
            "  inflating: /content/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g17_c05.avi  \n",
            "  inflating: /content/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g20_c02.avi  \n",
            "  inflating: /content/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g21_c01.avi  \n",
            "  inflating: /content/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g23_c03.avi  \n",
            "  inflating: /content/test/ApplyEyeMakeup/v_ApplyEyeMakeup_g24_c03.avi  \n",
            "Archive:  /content/train.zip\n",
            "   creating: /content/train/Archery/\n",
            "  inflating: /content/train/Archery/v_Archery_g01_c01.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g01_c02.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g01_c04.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g01_c05.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g01_c06.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g01_c07.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g02_c02.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g02_c04.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g02_c05.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g02_c06.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g02_c07.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g03_c01.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g03_c02.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g03_c03.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g03_c04.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g03_c05.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g04_c01.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g04_c02.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g04_c03.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g04_c04.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g04_c05.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g05_c01.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g05_c02.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g05_c03.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g05_c04.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g05_c05.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g06_c02.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g06_c03.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g06_c04.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g06_c05.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g06_c06.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g07_c01.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g07_c03.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g07_c04.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g07_c06.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g08_c02.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g08_c04.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g08_c05.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g09_c01.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g09_c04.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g09_c05.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g09_c06.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g09_c07.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g10_c01.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g10_c03.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g10_c04.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g10_c05.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g10_c07.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g11_c02.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g11_c04.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g11_c06.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g12_c01.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g12_c02.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g12_c03.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g13_c01.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g13_c02.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g13_c03.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g13_c04.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g13_c05.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g13_c07.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g14_c01.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g14_c02.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g14_c03.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g15_c01.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g15_c02.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g15_c03.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g15_c05.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g15_c06.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g16_c01.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g16_c02.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g16_c03.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g17_c01.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g17_c02.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g17_c03.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g18_c01.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g18_c03.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g18_c06.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g18_c07.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g19_c01.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g19_c02.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g19_c03.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g20_c01.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g20_c02.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g20_c03.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g20_c04.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g20_c05.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g20_c06.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g20_c07.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g21_c01.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g21_c04.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g22_c01.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g22_c02.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g22_c03.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g22_c04.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g23_c02.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g23_c03.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g23_c05.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g23_c06.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g23_c07.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g24_c01.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g24_c02.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g24_c03.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g24_c04.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g24_c05.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g25_c03.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g25_c04.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g25_c05.avi  \n",
            "  inflating: /content/train/Archery/v_Archery_g25_c07.avi  \n",
            "   creating: /content/train/BabyCrawling/\n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g01_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g01_c03.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g02_c01.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g02_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g02_c03.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g02_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g02_c05.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g03_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g03_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g04_c01.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g04_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g04_c03.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g04_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g05_c01.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g05_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g05_c03.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g05_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g05_c05.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g06_c01.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g06_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g06_c03.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g06_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g06_c05.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g06_c06.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g07_c01.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g07_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g07_c03.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g07_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g07_c06.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g08_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g08_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g09_c01.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g09_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g09_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g09_c05.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g09_c06.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g10_c01.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g10_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g10_c05.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g11_c01.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g11_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g11_c03.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g11_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g12_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g12_c03.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g12_c05.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g12_c06.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g13_c01.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g13_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g13_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g14_c01.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g14_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g14_c03.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g14_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g15_c01.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g15_c03.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g15_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g15_c05.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g16_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g16_c03.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g16_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g16_c05.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g16_c06.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g17_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g17_c03.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g17_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g18_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g18_c03.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g18_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g18_c05.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g18_c06.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g19_c01.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g19_c03.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g19_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g19_c05.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g20_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g20_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g20_c06.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g21_c01.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g21_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g21_c03.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g21_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g22_c01.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g22_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g22_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g22_c05.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g22_c07.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g23_c01.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g23_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g23_c03.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g23_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g24_c01.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g24_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g24_c03.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g24_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g25_c02.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g25_c04.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g25_c05.avi  \n",
            "  inflating: /content/train/BabyCrawling/v_BabyCrawling_g25_c06.avi  \n",
            "   creating: /content/train/BalanceBeam/\n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g01_c01.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g01_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g01_c03.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g01_c04.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g02_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g02_c03.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g03_c01.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g03_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g03_c03.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g03_c04.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g04_c01.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g04_c03.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g04_c04.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g05_c01.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g05_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g06_c03.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g06_c04.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g06_c05.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g06_c06.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g06_c07.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g07_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g07_c03.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g08_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g08_c03.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g08_c04.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g09_c01.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g09_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g10_c01.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g10_c03.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g11_c01.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g11_c03.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g12_c01.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g12_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g12_c03.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g12_c04.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g13_c01.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g13_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g13_c03.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g13_c04.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g14_c01.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g14_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g14_c03.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g14_c04.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g15_c01.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g15_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g15_c04.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g16_c01.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g16_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g16_c03.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g16_c04.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g17_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g17_c03.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g17_c04.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g17_c05.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g17_c06.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g18_c01.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g18_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g18_c03.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g18_c04.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g19_c01.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g19_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g19_c03.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g20_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g20_c03.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g20_c04.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g21_c01.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g21_c04.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g21_c05.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g22_c01.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g22_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g22_c04.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g23_c01.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g23_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g23_c04.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g23_c05.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g24_c01.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g24_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g24_c04.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g25_c01.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g25_c02.avi  \n",
            "  inflating: /content/train/BalanceBeam/v_BalanceBeam_g25_c04.avi  \n",
            "   creating: /content/train/ApplyEyeMakeup/\n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c06.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g02_c02.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g02_c03.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c01.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c02.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c03.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c04.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g03_c06.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g04_c01.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g04_c02.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g04_c03.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g04_c04.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g04_c05.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g04_c06.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c02.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c04.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g05_c06.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g06_c01.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g06_c03.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g06_c04.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g06_c05.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g06_c06.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c01.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c02.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c03.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c05.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c07.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c03.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c04.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c05.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c07.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g10_c02.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g10_c03.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g10_c04.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g10_c05.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g11_c01.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g11_c03.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g11_c04.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g11_c05.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g12_c01.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g12_c02.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g12_c03.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g12_c04.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g12_c06.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g13_c01.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g13_c02.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g13_c03.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g13_c04.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g13_c05.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g13_c06.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g14_c01.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g14_c03.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g14_c04.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c02.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c03.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c04.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c05.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g15_c06.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g16_c01.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g16_c02.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g16_c05.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g17_c01.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g17_c02.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g17_c04.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g18_c01.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g18_c03.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g18_c04.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g18_c05.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g19_c01.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g19_c02.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g19_c03.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g19_c04.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g20_c01.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g20_c03.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g20_c05.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g20_c06.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g21_c02.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g21_c03.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g21_c04.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g21_c05.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g22_c02.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g22_c03.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g22_c04.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g22_c05.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g23_c01.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g23_c02.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g23_c04.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g23_c05.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g23_c06.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g24_c01.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g24_c02.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g24_c04.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g24_c05.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g24_c07.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g25_c01.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g25_c02.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g25_c03.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g25_c04.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g25_c05.avi  \n",
            "  inflating: /content/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g25_c06.avi  \n",
            "   creating: /content/train/ApplyLipstick/\n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g01_c01.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g01_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g01_c03.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g02_c01.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g02_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g02_c03.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g02_c04.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g03_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g03_c03.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g03_c04.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g04_c01.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g04_c03.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g04_c04.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g04_c05.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g05_c01.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g05_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g05_c03.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g05_c04.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g05_c05.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g06_c01.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g06_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g06_c03.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g06_c04.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g07_c01.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g07_c03.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g07_c04.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g08_c01.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g08_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g08_c03.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g08_c04.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g09_c01.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g09_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g09_c03.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g10_c01.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g10_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g10_c04.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g11_c01.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g11_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g11_c04.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g12_c01.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g12_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g12_c05.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g13_c01.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g13_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g13_c03.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g14_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g14_c04.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g15_c03.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g15_c05.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g16_c01.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g16_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g16_c03.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g16_c04.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g17_c01.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g17_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g17_c03.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g17_c04.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g18_c01.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g18_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g18_c04.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g19_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g19_c03.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g19_c04.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g20_c01.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g20_c05.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g21_c01.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g21_c03.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g21_c04.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g21_c05.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g22_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g22_c03.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g22_c04.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g22_c05.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g22_c06.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g22_c07.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g23_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g23_c04.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g24_c01.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g24_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g24_c03.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g24_c05.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g25_c01.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g25_c02.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g25_c03.avi  \n",
            "  inflating: /content/train/ApplyLipstick/v_ApplyLipstick_g25_c04.avi  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9iH00g4wFrJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5eEAQ21t04K",
        "outputId": "54f2f426-ad05-444e-c5d7-830944651516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec  4 04:49:04 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21dNP8aItUVA",
        "outputId": "4ae14c11-ad88-4adf-b5bb-f300cf9f2475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpftZiEBuUIo",
        "outputId": "8b185921-0865-421a-a287-223270d33990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP4Y2SBBtYYy",
        "outputId": "7f4675ae-815e-4a74-e594-88d70c568606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.5.1+cu121\n",
            "CUDA available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load Data\n",
        "train_df = pd.read_csv(\"/content/Train.csv\")\n",
        "test_df = pd.read_csv(\"/content/Test.csv\")\n",
        "eval_df = pd.read_csv(\"/content/Val.csv\")\n",
        "\n",
        "# Update paths for Colab\n",
        "eval_df[\"clip_path\"] = \"/content/\" + eval_df[\"clip_path\"]\n",
        "test_df[\"clip_path\"] = \"/content/\" + test_df[\"clip_path\"]\n",
        "train_df[\"clip_path\"] = \"/content/\" + train_df[\"clip_path\"]\n",
        "\n",
        "# Step 2: Preprocess Labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_df[\"label_encoded\"] = label_encoder.fit_transform(train_df[\"label\"])\n",
        "test_df[\"label_encoded\"] = label_encoder.transform(test_df[\"label\"])\n",
        "eval_df[\"label_encoded\"] = label_encoder.transform(eval_df[\"label\"])\n",
        "\n",
        "train_paths = train_df[\"clip_path\"].values\n",
        "train_labels = train_df[\"label_encoded\"].values\n",
        "\n",
        "test_paths = test_df[\"clip_path\"].values\n",
        "test_labels = test_df[\"label_encoded\"].values\n",
        "\n",
        "eval_paths = eval_df[\"clip_path\"].values\n",
        "eval_labels = eval_df[\"label_encoded\"].values\n",
        "\n",
        "# Step 3: Dataset and DataLoader\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, paths, labels, transform=None, input_frames=10, pred_frames=5):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.input_frames = input_frames\n",
        "        self.pred_frames = pred_frames\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Extract frames\n",
        "        frames = self._extract_frames(video_path, self.input_frames + self.pred_frames)\n",
        "        if len(frames) < self.input_frames + self.pred_frames:\n",
        "            frames += [frames[-1]] * (self.input_frames + self.pred_frames - len(frames))\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            frames = [self.transform(frame) for frame in frames]\n",
        "        frames = torch.stack(frames)  # [T, C, H, W]\n",
        "\n",
        "        input_frames = frames[:self.input_frames]  # Input sequence\n",
        "        pred_frames = frames[self.input_frames:]  # Ground truth prediction\n",
        "        return input_frames, pred_frames\n",
        "\n",
        "    def _extract_frames(self, video_path, total_frames):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        step = max(1, frame_count // total_frames)\n",
        "        frames = []\n",
        "        for i in range(0, frame_count, step):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = Image.fromarray(frame)\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "        return frames[:total_frames]\n",
        "\n",
        "# Transformations (64x64 as required)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])  # Grayscale\n",
        "])\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = VideoDataset(train_paths, train_labels, transform=transform)\n",
        "test_dataset = VideoDataset(test_paths, test_labels, transform=transform)\n",
        "eval_dataset = VideoDataset(eval_paths, eval_labels, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Step 4: Define the ConvLSTM Model\n",
        "class ConvLSTM(nn.Module):\n",
        "    def __init__(self, input_frames, pred_frames, num_classes=1):\n",
        "        super(ConvLSTM, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(3, 32, kernel_size=(3, 3, 3), stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool3d((1, 2, 2))  # Reduces spatial dimensions (H, W) by half\n",
        "        self.lstm = nn.LSTM(input_size=32 * 32 * 32, hidden_size=256, batch_first=True)\n",
        "        self.fc = nn.Linear(256, pred_frames * 32 * 32 * 3)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)  # Restore to 64x64\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Rearrange to match Conv3D input expectations\n",
        "        x = x.permute(0, 2, 1, 3, 4)  # Shape: [batch_size, channels, depth, height, width]\n",
        "\n",
        "        # Convolutional layers\n",
        "        x = self.pool(self.relu(self.conv1(x)))  # Conv3D + Pooling\n",
        "\n",
        "        # Flatten for LSTM\n",
        "        b, c, t, h, w = x.size()\n",
        "        x = x.view(b, t, -1)  # Shape: [batch_size, depth, features]\n",
        "\n",
        "        # LSTM\n",
        "        x, _ = self.lstm(x)  # Shape: [batch_size, depth, hidden_size]\n",
        "        x = self.fc(x[:, -1, :])  # Use the last LSTM output\n",
        "\n",
        "        # Reshape and upsample\n",
        "        x = x.view(b, -1, 3, 32, 32)  # Shape: [batch_size, pred_frames, channels, 32, 32]\n",
        "        x = x.view(-1, 3, 32, 32)  # Flatten pred_frames for upsampling\n",
        "        x = self.upsample(x)  # Upsample to [batch_size * pred_frames, 3, 64, 64]\n",
        "        x = x.view(b, -1, 3, 64, 64)  # Restore batch and pred_frames structure\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss, optimizer\n",
        "input_frames = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pred_frames = 5\n",
        "model = ConvLSTM(input_frames, pred_frames).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Step 5: Train the Model\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=200, save_path=\"best_model.pth\"):\n",
        "    best_loss = float('inf')\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for input_frames, target_frames in train_loader:\n",
        "            input_frames, target_frames = input_frames.to(device), target_frames.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred_frames = model(input_frames)\n",
        "            loss = criterion(pred_frames, target_frames)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"Model saved to {save_path}\")\n",
        "\n",
        "# Step 6: Evaluate Model with MSE and SSIM\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train and Evaluate\n",
        "train_model(model, train_loader, criterion, optimizer, epochs=50)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vBbHKCc5KBF",
        "outputId": "1f9360e2-78cc-40aa-aa35-f1584669ffa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.1999\n",
            "Model saved to best_model.pth\n",
            "Epoch 2/50, Loss: 0.1438\n",
            "Model saved to best_model.pth\n",
            "Epoch 3/50, Loss: 0.1237\n",
            "Model saved to best_model.pth\n",
            "Epoch 4/50, Loss: 0.1119\n",
            "Model saved to best_model.pth\n",
            "Epoch 5/50, Loss: 0.1058\n",
            "Model saved to best_model.pth\n",
            "Epoch 6/50, Loss: 0.0983\n",
            "Model saved to best_model.pth\n",
            "Epoch 7/50, Loss: 0.0945\n",
            "Model saved to best_model.pth\n",
            "Epoch 8/50, Loss: 0.0902\n",
            "Model saved to best_model.pth\n",
            "Epoch 9/50, Loss: 0.0859\n",
            "Model saved to best_model.pth\n",
            "Epoch 10/50, Loss: 0.0830\n",
            "Model saved to best_model.pth\n",
            "Epoch 11/50, Loss: 0.0807\n",
            "Model saved to best_model.pth\n",
            "Epoch 12/50, Loss: 0.0770\n",
            "Model saved to best_model.pth\n",
            "Epoch 13/50, Loss: 0.0737\n",
            "Model saved to best_model.pth\n",
            "Epoch 14/50, Loss: 0.0709\n",
            "Model saved to best_model.pth\n",
            "Epoch 15/50, Loss: 0.0687\n",
            "Model saved to best_model.pth\n",
            "Epoch 16/50, Loss: 0.0672\n",
            "Model saved to best_model.pth\n",
            "Epoch 17/50, Loss: 0.0643\n",
            "Model saved to best_model.pth\n",
            "Epoch 18/50, Loss: 0.0627\n",
            "Model saved to best_model.pth\n",
            "Epoch 19/50, Loss: 0.0606\n",
            "Model saved to best_model.pth\n",
            "Epoch 20/50, Loss: 0.0591\n",
            "Model saved to best_model.pth\n",
            "Epoch 21/50, Loss: 0.0570\n",
            "Model saved to best_model.pth\n",
            "Epoch 22/50, Loss: 0.0565\n",
            "Model saved to best_model.pth\n",
            "Epoch 23/50, Loss: 0.0552\n",
            "Model saved to best_model.pth\n",
            "Epoch 24/50, Loss: 0.0529\n",
            "Model saved to best_model.pth\n",
            "Epoch 25/50, Loss: 0.0515\n",
            "Model saved to best_model.pth\n",
            "Epoch 26/50, Loss: 0.0505\n",
            "Model saved to best_model.pth\n",
            "Epoch 27/50, Loss: 0.0502\n",
            "Model saved to best_model.pth\n",
            "Epoch 28/50, Loss: 0.0492\n",
            "Model saved to best_model.pth\n",
            "Epoch 29/50, Loss: 0.0480\n",
            "Model saved to best_model.pth\n",
            "Epoch 30/50, Loss: 0.0469\n",
            "Model saved to best_model.pth\n",
            "Epoch 31/50, Loss: 0.0455\n",
            "Model saved to best_model.pth\n",
            "Epoch 32/50, Loss: 0.0445\n",
            "Model saved to best_model.pth\n",
            "Epoch 33/50, Loss: 0.0438\n",
            "Model saved to best_model.pth\n",
            "Epoch 34/50, Loss: 0.0423\n",
            "Model saved to best_model.pth\n",
            "Epoch 35/50, Loss: 0.0418\n",
            "Model saved to best_model.pth\n",
            "Epoch 36/50, Loss: 0.0430\n",
            "Epoch 37/50, Loss: 0.0415\n",
            "Model saved to best_model.pth\n",
            "Epoch 38/50, Loss: 0.0502\n",
            "Epoch 39/50, Loss: 0.0416\n",
            "Epoch 40/50, Loss: 0.0386\n",
            "Model saved to best_model.pth\n",
            "Epoch 41/50, Loss: 0.0382\n",
            "Model saved to best_model.pth\n",
            "Epoch 42/50, Loss: 0.0370\n",
            "Model saved to best_model.pth\n",
            "Epoch 43/50, Loss: 0.0364\n",
            "Model saved to best_model.pth\n",
            "Epoch 44/50, Loss: 0.0359\n",
            "Model saved to best_model.pth\n",
            "Epoch 45/50, Loss: 0.0364\n",
            "Epoch 46/50, Loss: 0.0380\n",
            "Epoch 47/50, Loss: 0.0361\n",
            "Epoch 48/50, Loss: 0.0345\n",
            "Model saved to best_model.pth\n",
            "Epoch 49/50, Loss: 0.0331\n",
            "Model saved to best_model.pth\n",
            "Epoch 50/50, Loss: 0.0324\n",
            "Model saved to best_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VAT072S-bun1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load Data\n",
        "train_df = pd.read_csv(\"/content/Train.csv\")\n",
        "test_df = pd.read_csv(\"/content/Test.csv\")\n",
        "eval_df = pd.read_csv(\"/content/Val.csv\")\n",
        "\n",
        "# Update paths for Colab\n",
        "eval_df[\"clip_path\"] = \"/content/\" + eval_df[\"clip_path\"]\n",
        "test_df[\"clip_path\"] = \"/content/\" + test_df[\"clip_path\"]\n",
        "train_df[\"clip_path\"] = \"/content/\" + train_df[\"clip_path\"]\n",
        "\n",
        "# Step 2: Dataset and DataLoader\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, paths, labels, transform=None, input_frames=10, pred_frames=5):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.input_frames = input_frames\n",
        "        self.pred_frames = pred_frames\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        frames = self._extract_frames(video_path, self.input_frames + self.pred_frames)\n",
        "        if len(frames) < self.input_frames + self.pred_frames:\n",
        "            frames += [frames[-1]] * (self.input_frames + self.pred_frames - len(frames))\n",
        "        if self.transform:\n",
        "            frames = [self.transform(frame) for frame in frames]\n",
        "        frames = torch.stack(frames)  # [T, C, H, W]\n",
        "        input_frames = frames[:self.input_frames]\n",
        "        pred_frames = frames[self.input_frames:]\n",
        "        return input_frames, pred_frames\n",
        "\n",
        "    def _extract_frames(self, video_path, total_frames):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        step = max(1, frame_count // total_frames)\n",
        "        frames = []\n",
        "        for i in range(0, frame_count, step):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = Image.fromarray(frame)\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "        return frames[:total_frames]\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "# Step 2: Preprocess Labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_df[\"label_encoded\"] = label_encoder.fit_transform(train_df[\"label\"])\n",
        "test_df[\"label_encoded\"] = label_encoder.transform(test_df[\"label\"])\n",
        "eval_df[\"label_encoded\"] = label_encoder.transform(eval_df[\"label\"])\n",
        "\n",
        "train_paths = train_df[\"clip_path\"].values\n",
        "train_labels = train_df[\"label_encoded\"].values\n",
        "\n",
        "test_paths = test_df[\"clip_path\"].values\n",
        "test_labels = test_df[\"label_encoded\"].values\n",
        "\n",
        "eval_paths = eval_df[\"clip_path\"].values\n",
        "eval_labels = eval_df[\"label_encoded\"].values\n",
        "\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = VideoDataset(train_paths, train_labels, transform=transform)\n",
        "test_dataset = VideoDataset(test_paths, test_labels, transform=transform)\n",
        "eval_dataset = VideoDataset(eval_paths, eval_labels, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=4, shuffle=False)\n"
      ],
      "metadata": {
        "id": "emBK-AwoGinR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Class mappings:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_U2WGAyuuHW",
        "outputId": "281c270e-9920-419d-b219-548ebc3f207b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class mappings: {'ApplyEyeMakeup': 0, 'ApplyLipstick': 1, 'Archery': 2, 'BabyCrawling': 3, 'BalanceBeam': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "target_class = \"Archery\"\n",
        "class_idx = label_encoder.transform([target_class])[0]\n",
        "filtered_df = eval_df[eval_df[\"label_encoded\"] == class_idx]\n",
        "\n",
        "# Extract paths and labels for this class\n",
        "filtered_paths = filtered_df[\"clip_path\"].values\n",
        "filtered_labels = filtered_df[\"label_encoded\"].values\n"
      ],
      "metadata": {
        "id": "mYK-e0I3u4cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_dataset = VideoDataset(filtered_paths, filtered_labels, transform=transform)\n",
        "filtered_loader = DataLoader(filtered_dataset, batch_size=4, shuffle=False)\n"
      ],
      "metadata": {
        "id": "gzx9T8X0vAz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_video(frames, save_path=\"generated_video.mp4\", fps=10):\n",
        "    # Check input frames\n",
        "    if len(frames) == 0:\n",
        "        print(\"Error: No frames to write!\")\n",
        "        return\n",
        "    if frames.max() <= 1.0:\n",
        "        frames = np.clip((frames * 255), 0, 255).astype(np.uint8)  # Scale to [0, 255]\n",
        "\n",
        "    # Ensure frames are in [T, H, W, C] format\n",
        "    print(f\"Frames to write: {len(frames)}, Resolution: {frames[0].shape[:2]}\")\n",
        "\n",
        "    height, width = frames[0].shape[:2]\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    video_writer = cv2.VideoWriter(save_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Write frames to video\n",
        "    for i, frame in enumerate(frames):\n",
        "        print(f\"Writing frame {i+1}/{len(frames)}\")\n",
        "        video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))  # OpenCV expects BGR\n",
        "\n",
        "    video_writer.release()\n",
        "    print(f\"Video saved to {save_path}\")\n",
        "# Load trained model\n",
        "# Load trained model\n",
        "model.load_state_dict(torch.load(\"/content/best_model.pth\"))# Evaluate on filtered class\n",
        "model.eval()\n",
        "for batch_idx, (input_frames, target_frames) in enumerate(filtered_loader):\n",
        "    input_frames = input_frames.to(device)\n",
        "    with torch.no_grad():\n",
        "        pred_frames_batch = model(input_frames).cpu().numpy()  # [batch_size, T, C, H, W]\n",
        "\n",
        "    # Save videos\n",
        "    for i, pred_frames in enumerate(pred_frames_batch):\n",
        "        # Process the predicted frames\n",
        "        pred_frames = pred_frames.transpose(0, 2, 3, 1)  # [T, H, W, C]\n",
        "        pred_frames = (pred_frames * 0.5) + 0.5  # Convert to [0, 1]\n",
        "        pred_frames = np.clip(pred_frames, 0, 1)\n",
        "\n",
        "        # Save the video\n",
        "        save_path = f\"predicted_{target_class}_video_{batch_idx*4 + i + 1}.mp4\"\n",
        "        generate_video(pred_frames, save_path=save_path, fps=10)\n",
        "        print(f\"Saved: {save_path}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gqdXcTS7mkE",
        "outputId": "e52a8616-a585-4420-e3a2-51e6369d9c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-308fd41d789c>:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/best_model.pth\"))# Evaluate on filtered class\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_1.mp4\n",
            "Saved: predicted_Archery_video_1.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_2.mp4\n",
            "Saved: predicted_Archery_video_2.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_3.mp4\n",
            "Saved: predicted_Archery_video_3.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_4.mp4\n",
            "Saved: predicted_Archery_video_4.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_5.mp4\n",
            "Saved: predicted_Archery_video_5.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_6.mp4\n",
            "Saved: predicted_Archery_video_6.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_7.mp4\n",
            "Saved: predicted_Archery_video_7.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_8.mp4\n",
            "Saved: predicted_Archery_video_8.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_9.mp4\n",
            "Saved: predicted_Archery_video_9.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_10.mp4\n",
            "Saved: predicted_Archery_video_10.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_11.mp4\n",
            "Saved: predicted_Archery_video_11.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_12.mp4\n",
            "Saved: predicted_Archery_video_12.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_13.mp4\n",
            "Saved: predicted_Archery_video_13.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_14.mp4\n",
            "Saved: predicted_Archery_video_14.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_15.mp4\n",
            "Saved: predicted_Archery_video_15.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_16.mp4\n",
            "Saved: predicted_Archery_video_16.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_17.mp4\n",
            "Saved: predicted_Archery_video_17.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_18.mp4\n",
            "Saved: predicted_Archery_video_18.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def evaluate_model(model, data_loader, device, load_path=None):\n",
        "    if load_path:  # Load model if a path is provided\n",
        "        model.load_state_dict(torch.load(load_path, map_location=device))\n",
        "        print(f\"Model loaded from {load_path}\")\n",
        "\n",
        "    model.eval()\n",
        "    total_mse, total_ssim = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_frames, target_frames in data_loader:\n",
        "            input_frames, target_frames = input_frames.to(device), target_frames.to(device)\n",
        "            pred_frames = model(input_frames).cpu().numpy()  # [batch_size, pred_frames, channels, height, width]\n",
        "            target_frames = target_frames.cpu().numpy()\n",
        "\n",
        "            # Iterate over batch\n",
        "            for pred_seq, target_seq in zip(pred_frames, target_frames):  # [pred_frames, channels, height, width]\n",
        "                for pred, target in zip(pred_seq, target_seq):  # Iterate over individual frames\n",
        "                    # Normalize and convert to [0, 1]\n",
        "                    pred = np.clip((pred * 0.5) + 0.5, 0, 1)\n",
        "                    target = np.clip((target * 0.5) + 0.5, 0, 1)\n",
        "\n",
        "                    # Transpose to (height, width, channels) for SSIM\n",
        "                    pred = pred.transpose(1, 2, 0)\n",
        "                    target = target.transpose(1, 2, 0)\n",
        "\n",
        "                    # Calculate MSE\n",
        "                    mse = ((pred - target) ** 2).mean()\n",
        "                    total_mse += mse\n",
        "\n",
        "                    # Adjust win_size dynamically for SSIM\n",
        "                    win_size = min(7, pred.shape[0], pred.shape[1])  # Ensure win_size fits dimensions\n",
        "                    if win_size % 2 == 0:  # Ensure odd win_size\n",
        "                        win_size -= 1\n",
        "\n",
        "                    # Calculate SSIM\n",
        "                    ssim_value = ssim(\n",
        "                        pred, target,\n",
        "                        data_range=1.0,  # Adjust based on normalization\n",
        "                        win_size=win_size,\n",
        "                        channel_axis=-1  # Specify channel axis\n",
        "                    )\n",
        "                    total_ssim += ssim_value\n",
        "\n",
        "    # Compute averages\n",
        "    num_frames = len(data_loader.dataset) * pred_frames.shape[1]  # Total number of frames\n",
        "    avg_mse = total_mse / num_frames\n",
        "    avg_ssim = total_ssim / num_frames\n",
        "\n",
        "    print(f\"Evaluation Results: MSE: {avg_mse:.4f}, SSIM: {avg_ssim:.4f}\")\n",
        "\n",
        "# After training, evaluate the model on the test or validation dataset\n",
        "evaluate_model(model, eval_loader, device, load_path=\"best_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpL6V1xlYSbQ",
        "outputId": "22235262-ff76-45f3-9ea0-e470a8ffaec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-f6d14f34a671>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(load_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from best_model.pth\n",
            "Evaluation Results: MSE: 0.0158, SSIM: 0.4520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def evaluate_model(model, data_loader, device, dataset_name=\"Validation\", load_path=None):\n",
        "    \"\"\"\n",
        "    Evaluate the model on a given dataset loader.\n",
        "\n",
        "    Args:\n",
        "    - model: PyTorch model to be evaluated.\n",
        "    - data_loader: DataLoader for the dataset.\n",
        "    - device: PyTorch device (e.g., \"cuda\" or \"cpu\").\n",
        "    - dataset_name: String indicating the dataset type (e.g., \"Train\", \"Test\").\n",
        "    - load_path: Path to load a pretrained model. Default is None.\n",
        "\n",
        "    Returns:\n",
        "    - avg_mse: Average Mean Squared Error over the dataset.\n",
        "    - avg_ssim: Average Structural Similarity Index over the dataset.\n",
        "    \"\"\"\n",
        "    if load_path:  # Load model if a path is provided\n",
        "        model.load_state_dict(torch.load(load_path, map_location=device))\n",
        "        print(f\"Model loaded from {load_path}\")\n",
        "\n",
        "    model.eval()\n",
        "    total_mse, total_ssim = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_frames, target_frames in data_loader:\n",
        "            input_frames, target_frames = input_frames.to(device), target_frames.to(device)\n",
        "            pred_frames = model(input_frames).cpu().numpy()  # [batch_size, pred_frames, channels, height, width]\n",
        "            target_frames = target_frames.cpu().numpy()\n",
        "\n",
        "            # Iterate over batch\n",
        "            for pred_seq, target_seq in zip(pred_frames, target_frames):  # [pred_frames, channels, height, width]\n",
        "                for pred, target in zip(pred_seq, target_seq):  # Iterate over individual frames\n",
        "                    # Normalize and convert to [0, 1]\n",
        "                    pred = np.clip((pred * 0.5) + 0.5, 0, 1)\n",
        "                    target = np.clip((target * 0.5) + 0.5, 0, 1)\n",
        "\n",
        "                    # Transpose to (height, width, channels) for SSIM\n",
        "                    pred = pred.transpose(1, 2, 0)\n",
        "                    target = target.transpose(1, 2, 0)\n",
        "\n",
        "                    # Calculate MSE\n",
        "                    mse = ((pred - target) ** 2).mean()\n",
        "                    total_mse += mse\n",
        "\n",
        "                    # Adjust win_size dynamically for SSIM\n",
        "                    win_size = min(7, pred.shape[0], pred.shape[1])  # Ensure win_size fits dimensions\n",
        "                    if win_size % 2 == 0:  # Ensure odd win_size\n",
        "                        win_size -= 1\n",
        "\n",
        "                    # Calculate SSIM\n",
        "                    ssim_value = ssim(\n",
        "                        pred, target,\n",
        "                        data_range=1.0,  # Adjust based on normalization\n",
        "                        win_size=win_size,\n",
        "                        channel_axis=-1  # Specify channel axis\n",
        "                    )\n",
        "                    total_ssim += ssim_value\n",
        "\n",
        "    # Compute averages\n",
        "    num_frames = len(data_loader.dataset) * pred_frames.shape[1]  # Total number of frames\n",
        "    avg_mse = total_mse / num_frames\n",
        "    avg_ssim = total_ssim / num_frames\n",
        "\n",
        "    print(f\"{dataset_name} Results: MSE: {avg_mse:.4f}, SSIM: {avg_ssim:.4f}\")\n",
        "    return avg_mse, avg_ssim\n",
        "\n",
        "# Evaluate the model on train and test datasets\n",
        "train_results = evaluate_model(model, train_loader, device, dataset_name=\"Train\", load_path=\"best_model.pth\")\n",
        "test_results = evaluate_model(model, test_loader, device, dataset_name=\"Test\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc6VjLYVtrNC",
        "outputId": "62705709-5940-4d39-ccb2-5d30a94c23fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-1e2411b55f86>:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(load_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from best_model.pth\n",
            "Train Results: MSE: 0.0076, SSIM: 0.6183\n",
            "Test Results: MSE: 0.0171, SSIM: 0.4278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def evaluate_model(model, data_loader, device, dataset_name=\"Validation\", load_path=None):\n",
        "    \"\"\"\n",
        "    Evaluate the model on a given dataset loader.\n",
        "\n",
        "    Args:\n",
        "    - model: PyTorch model to be evaluated.\n",
        "    - data_loader: DataLoader for the dataset.\n",
        "    - device: PyTorch device (e.g., \"cuda\" or \"cpu\").\n",
        "    - dataset_name: String indicating the dataset type (e.g., \"Train\", \"Test\").\n",
        "    - load_path: Path to load a pretrained model. Default is None.\n",
        "\n",
        "    Returns:\n",
        "    - avg_mse: Average Mean Squared Error over the dataset.\n",
        "    - avg_ssim: Average Structural Similarity Index over the dataset.\n",
        "    \"\"\"\n",
        "    if load_path:  # Load model if a path is provided\n",
        "        model.load_state_dict(torch.load(load_path, map_location=device))\n",
        "        print(f\"Model loaded from {load_path}\")\n",
        "\n",
        "    model.eval()\n",
        "    total_mse, total_ssim = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_frames, target_frames in data_loader:\n",
        "            input_frames, target_frames = input_frames.to(device), target_frames.to(device)\n",
        "            pred_frames = model(input_frames).cpu().numpy()  # [batch_size, pred_frames, channels, height, width]\n",
        "            target_frames = target_frames.cpu().numpy()\n",
        "\n",
        "            # Iterate over batch\n",
        "            for pred_seq, target_seq in zip(pred_frames, target_frames):  # [pred_frames, channels, height, width]\n",
        "                for pred, target in zip(pred_seq, target_seq):  # Iterate over individual frames\n",
        "                    # Normalize and convert to [0, 1]\n",
        "                    pred = np.clip((pred * 0.5) + 0.5, 0, 1)\n",
        "                    target = np.clip((target * 0.5) + 0.5, 0, 1)\n",
        "\n",
        "                    # Transpose to (height, width, channels) for SSIM\n",
        "                    pred = pred.transpose(1, 2, 0)\n",
        "                    target = target.transpose(1, 2, 0)\n",
        "\n",
        "                    # Calculate MSE\n",
        "                    mse = ((pred - target) ** 2).mean()\n",
        "                    total_mse += mse\n",
        "\n",
        "                    # Adjust win_size dynamically for SSIM\n",
        "                    win_size = min(7, pred.shape[0], pred.shape[1])  # Ensure win_size fits dimensions\n",
        "                    if win_size % 2 == 0:  # Ensure odd win_size\n",
        "                        win_size -= 1\n",
        "\n",
        "                    # Calculate SSIM\n",
        "                    ssim_value = ssim(\n",
        "                        pred, target,\n",
        "                        data_range=1.0,  # Adjust based on normalization\n",
        "                        win_size=win_size,\n",
        "                        channel_axis=-1  # Specify channel axis\n",
        "                    )\n",
        "                    total_ssim += ssim_value\n",
        "\n",
        "    # Compute averages\n",
        "    num_frames = len(data_loader.dataset) * pred_frames.shape[1]  # Total number of frames\n",
        "    avg_mse = total_mse / num_frames\n",
        "    avg_ssim = total_ssim / num_frames\n",
        "\n",
        "    print(f\"{dataset_name} Results: MSE: {avg_mse:.4f}, SSIM: {avg_ssim:.4f}\")\n",
        "    return avg_mse, avg_ssim\n",
        "\n",
        "# Evaluate the model on train, validation, and test datasets\n",
        "train_results = evaluate_model(model, train_loader, device, dataset_name=\"Train\", load_path=\"best_transformer_model.pth\")\n",
        "val_results = evaluate_model(model, eval_loader, device, dataset_name=\"Validation\", load_path=\"best_transformer_model.pth\")\n",
        "test_results = evaluate_model(model, test_loader, device, dataset_name=\"Test\")\n"
      ],
      "metadata": {
        "id": "f5d4L0jrxliO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_input_and_predicted_frames(input_frames, pred_frames):\n",
        "    \"\"\"\n",
        "    Combine input frames and predicted frames into a single sequence for video generation.\n",
        "    Args:\n",
        "        input_frames: The original input frames (tensor of shape [T, C, H, W]).\n",
        "        pred_frames: The predicted frames (tensor of shape [T, C, H, W]).\n",
        "\n",
        "    Returns:\n",
        "        combined_frames: A numpy array of combined frames in [T, H, W, C] format.\n",
        "    \"\"\"\n",
        "    # Transpose input frames to [T, H, W, C] format\n",
        "    input_frames = input_frames.permute(0, 2, 3, 1).cpu().numpy()  # [T, H, W, C]\n",
        "\n",
        "    # Transpose predicted frames to [T, H, W, C] format\n",
        "    pred_frames = pred_frames.permute(0, 2, 3, 1).cpu().numpy()  # [T, H, W, C]\n",
        "\n",
        "    # Combine the input and predicted frames\n",
        "    combined_frames = np.concatenate((input_frames, pred_frames), axis=0)  # Concatenate along time axis\n",
        "\n",
        "    # Unnormalize and clip the frames\n",
        "    combined_frames = (combined_frames * 0.5) + 0.5  # Convert to [0, 1]\n",
        "    combined_frames = np.clip(combined_frames, 0, 1)  # Ensure valid range\n",
        "\n",
        "    return combined_frames\n",
        "# Load trained model\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "# Get a batch from the evaluation set\n",
        "input_frames, target_frames = next(iter(eval_loader))\n",
        "input_frames = input_frames.to(device)\n",
        "\n",
        "# Predict frames\n",
        "with torch.no_grad():\n",
        "    pred_frames = model(input_frames)\n",
        "    print(f\"Predicted frames shape: {pred_frames.shape}\")  # [4, 5, 3, 64, 64]\n",
        "\n",
        "# Select the first video in the batch\n",
        "input_frames_single = input_frames[0]  # Shape: [10, 3, 64, 64] for the first video\n",
        "pred_frames_single = pred_frames[0]  # Shape: [5, 3, 64, 64]\n",
        "\n",
        "# Combine input and predicted frames\n",
        "combined_frames = combine_input_and_predicted_frames(input_frames_single, pred_frames_single)\n",
        "\n",
        "# Debug frame properties\n",
        "print(f\"Combined frames shape: {combined_frames.shape}\")\n",
        "print(f\"Pixel value range after clipping: {combined_frames.min()} to {combined_frames.max()}\")\n",
        "\n",
        "# Generate video from combined frames\n",
        "generate_video(combined_frames, save_path=\"combined_video.mp4\", fps=10)\n",
        "\n",
        "# Display video in Colab\n",
        "from IPython.display import Video\n",
        "Video(\"combined_video.mp4\", embed=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "ynyQUZOJkwD0",
        "outputId": "72e30345-21f1-45a0-f2a0-bfdc9198e759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-22ec34e129d4>:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted frames shape: torch.Size([4, 5, 3, 64, 64])\n",
            "Combined frames shape: (15, 64, 64, 3)\n",
            "Pixel value range after clipping: 0.0 to 0.9960784316062927\n",
            "Frames to write: 15, Resolution: (64, 64)\n",
            "Writing frame 1/15\n",
            "Writing frame 2/15\n",
            "Writing frame 3/15\n",
            "Writing frame 4/15\n",
            "Writing frame 5/15\n",
            "Writing frame 6/15\n",
            "Writing frame 7/15\n",
            "Writing frame 8/15\n",
            "Writing frame 9/15\n",
            "Writing frame 10/15\n",
            "Writing frame 11/15\n",
            "Writing frame 12/15\n",
            "Writing frame 13/15\n",
            "Writing frame 14/15\n",
            "Writing frame 15/15\n",
            "Video saved to combined_video.mp4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video controls  >\n",
              " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              " Your browser does not support the video tag.\n",
              " </video>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRED RNN"
      ],
      "metadata": {
        "id": "he-W6bcz7qXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the SpatioTemporalLSTMCell\n",
        "class SpatioTemporalLSTMCell(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(SpatioTemporalLSTMCell, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Note: input_dim only applies to the first layer; other layers use hidden_dim.\n",
        "        self.input_to_state = nn.Conv2d(hidden_dim + input_dim, hidden_dim * 4, kernel_size=3, padding=1)\n",
        "        self.state_to_memory = nn.Conv2d(hidden_dim * 2, hidden_dim, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, input_tensor, hidden_state, cell_state, memory):\n",
        "        # Concatenate input and hidden state\n",
        "        combined = torch.cat([input_tensor, hidden_state], dim=1)  # Shape: [batch, input_dim + hidden_dim, H, W]\n",
        "\n",
        "        # Compute gates\n",
        "        gates = self.input_to_state(combined)  # Shape: [batch, hidden_dim * 4, H, W]\n",
        "        i, f, o, g = torch.split(gates, self.hidden_dim, dim=1)  # Split into input, forget, output, and candidate gates\n",
        "        i = torch.sigmoid(i)\n",
        "        f = torch.sigmoid(f)\n",
        "        o = torch.sigmoid(o)\n",
        "        g = torch.tanh(g)\n",
        "\n",
        "        # Update cell state and hidden state\n",
        "        cell_state = f * cell_state + i * g\n",
        "        hidden_state = o * torch.tanh(cell_state)\n",
        "\n",
        "        # Update memory\n",
        "        memory_input = torch.cat([hidden_state, cell_state], dim=1)\n",
        "        memory = torch.tanh(self.state_to_memory(memory_input)) + memory\n",
        "\n",
        "        return hidden_state, cell_state, memory\n",
        "\n",
        "class PredRNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, input_frames, pred_frames, height, width):\n",
        "        super(PredRNN, self).__init__()\n",
        "        self.input_frames = input_frames\n",
        "        self.pred_frames = pred_frames\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "        # Initialize layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            in_channels = input_dim if i == 0 else hidden_dim\n",
        "            self.layers.append(SpatioTemporalLSTMCell(in_channels, hidden_dim))\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Conv2d(hidden_dim, input_dim, kernel_size=1)  # Final output to reconstruct frames\n",
        "\n",
        "    def forward(self, input_frames):\n",
        "        batch_size, seq_len, channels, height, width = input_frames.size()\n",
        "        assert height == self.height and width == self.width, \"Frame dimensions must match the model's height and width.\"\n",
        "\n",
        "        # Initialize hidden states, cell states, and memory for each layer\n",
        "        hidden_states = [torch.zeros(batch_size, layer.hidden_dim, self.height, self.width).to(input_frames.device)\n",
        "                         for layer in self.layers]\n",
        "        cell_states = [torch.zeros(batch_size, layer.hidden_dim, self.height, self.width).to(input_frames.device)\n",
        "                       for layer in self.layers]\n",
        "        memory = torch.zeros(batch_size, self.layers[0].hidden_dim, self.height, self.width).to(input_frames.device)\n",
        "\n",
        "        # Process input frames\n",
        "        for t in range(self.input_frames):\n",
        "            x = input_frames[:, t]  # Extract frame at timestep t\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                hidden_states[i], cell_states[i], memory = layer(\n",
        "                    x if i == 0 else hidden_states[i - 1],  # Use input only for the first layer\n",
        "                    hidden_states[i],\n",
        "                    cell_states[i],\n",
        "                    memory\n",
        "                )\n",
        "\n",
        "        # Generate predicted frames\n",
        "        outputs = []\n",
        "        for t in range(self.pred_frames):\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                hidden_states[i], cell_states[i], memory = layer(\n",
        "                    x if i == 0 else hidden_states[i - 1],  # Use output of previous layer\n",
        "                    hidden_states[i],\n",
        "                    cell_states[i],\n",
        "                    memory\n",
        "                )\n",
        "            outputs.append(self.output_layer(hidden_states[-1]))  # Generate the output frame from the last layer\n",
        "\n",
        "        return torch.stack(outputs, dim=1)  # Shape: [batch_size, pred_frames, channels, height, width]\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),  # Resize to 64x64 pixels\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])  # Normalize to mean 0.5 and std 0.5\n",
        "])\n",
        "for input_frames, target_frames in train_loader:\n",
        "    print(f\"Input frames: {input_frames.shape}, Target frames: {target_frames.shape}\")\n",
        "    break\n",
        "\n",
        "\n",
        "# Initialize the PredRNN model\n",
        "input_frames = 10\n",
        "pred_frames = 5\n",
        "input_dim = 3  # RGB channels\n",
        "hidden_dim = 64  # Hidden state dimension\n",
        "num_layers = 2  # Number of ST-LSTM layers\n",
        "height, width = 64, 64  # Frame dimensions\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PredRNN(input_dim, hidden_dim, num_layers, input_frames, pred_frames, height, width).to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=50, save_path=\"best_predrnn_model.pth\"):\n",
        "    best_loss = float('inf')\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for input_frames, target_frames in train_loader:\n",
        "            input_frames, target_frames = input_frames.to(device), target_frames.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred_frames = model(input_frames)\n",
        "            loss = criterion(pred_frames, target_frames)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"Model saved to {save_path}\")\n",
        "for input_frames, target_frames in train_loader:\n",
        "    print(f\"Input frames: {input_frames.shape}, Target frames: {target_frames.shape}\")\n",
        "    break\n",
        "\n",
        "# Train the PredRNN model\n",
        "#train_model(model, train_loader, criterion, optimizer, epochs=50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0mB5kY_7o2f",
        "outputId": "6c8ac26e-eca5-44d4-abfb-5684e074fd44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input frames: torch.Size([4, 10, 3, 64, 64]), Target frames: torch.Size([4, 5, 3, 64, 64])\n",
            "Input frames: torch.Size([4, 10, 3, 64, 64]), Target frames: torch.Size([4, 5, 3, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "def evaluate_model(model, data_loader, device, dataset_name=\"Validation\", load_path=None):\n",
        "    \"\"\"\n",
        "    Evaluate the model on a given dataset loader.\n",
        "\n",
        "    Args:\n",
        "    - model: PyTorch model to be evaluated.\n",
        "    - data_loader: DataLoader for the dataset.\n",
        "    - device: PyTorch device (e.g., \"cuda\" or \"cpu\").\n",
        "    - dataset_name: String indicating the dataset type (e.g., \"Train\", \"Test\").\n",
        "    - load_path: Path to load a pretrained model. Default is None.\n",
        "\n",
        "    Returns:\n",
        "    - avg_mse: Average Mean Squared Error over the dataset.\n",
        "    - avg_ssim: Average Structural Similarity Index over the dataset.\n",
        "    \"\"\"\n",
        "    if load_path:  # Load model if a path is provided\n",
        "        model.load_state_dict(torch.load(load_path, map_location=device))\n",
        "        print(f\"Model loaded from {load_path}\")\n",
        "\n",
        "    model.eval()\n",
        "    total_mse, total_ssim = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_frames, target_frames in data_loader:\n",
        "            input_frames, target_frames = input_frames.to(device), target_frames.to(device)\n",
        "            pred_frames = model(input_frames).cpu().numpy()  # [batch_size, pred_frames, channels, height, width]\n",
        "            target_frames = target_frames.cpu().numpy()\n",
        "\n",
        "            # Iterate over batch\n",
        "            for pred_seq, target_seq in zip(pred_frames, target_frames):  # [pred_frames, channels, height, width]\n",
        "                for pred, target in zip(pred_seq, target_seq):  # Iterate over individual frames\n",
        "                    # Normalize and convert to [0, 1]\n",
        "                    pred = np.clip((pred * 0.5) + 0.5, 0, 1)\n",
        "                    target = np.clip((target * 0.5) + 0.5, 0, 1)\n",
        "\n",
        "                    # Transpose to (height, width, channels) for SSIM\n",
        "                    pred = pred.transpose(1, 2, 0)\n",
        "                    target = target.transpose(1, 2, 0)\n",
        "\n",
        "                    # Calculate MSE\n",
        "                    mse = ((pred - target) ** 2).mean()\n",
        "                    total_mse += mse\n",
        "\n",
        "                    # Adjust win_size dynamically for SSIM\n",
        "                    win_size = min(7, pred.shape[0], pred.shape[1])  # Ensure win_size fits dimensions\n",
        "                    if win_size % 2 == 0:  # Ensure odd win_size\n",
        "                        win_size -= 1\n",
        "\n",
        "                    # Calculate SSIM\n",
        "                    ssim_value = ssim(\n",
        "                        pred, target,\n",
        "                        data_range=1.0,  # Adjust based on normalization\n",
        "                        win_size=win_size,\n",
        "                        channel_axis=-1  # Specify channel axis\n",
        "                    )\n",
        "                    total_ssim += ssim_value\n",
        "\n",
        "    # Compute averages\n",
        "    num_frames = len(data_loader.dataset) * pred_frames.shape[1]  # Total number of frames\n",
        "    avg_mse = total_mse / num_frames\n",
        "    avg_ssim = total_ssim / num_frames\n",
        "\n",
        "    print(f\"{dataset_name} Results: MSE: {avg_mse:.4f}, SSIM: {avg_ssim:.4f}\")\n",
        "    return avg_mse, avg_ssim\n",
        "\n",
        "# Evaluate the model on train, validation, and test datasets\n",
        "train_results = evaluate_model(model, train_loader, device, dataset_name=\"Train\", load_path=\"best_predrnn_model.pth\")\n",
        "val_results = evaluate_model(model, eval_loader, device, dataset_name=\"Validation\", load_path=\"best_predrnn_model.pth\")\n",
        "test_results = evaluate_model(model, test_loader, device, dataset_name=\"Test\")"
      ],
      "metadata": {
        "id": "B2b6VEN5Bnqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1774d1df-e465-40bd-87dd-3bb43fac93cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-8cb43ce010a6>:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(load_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from best_predrnn_model.pth\n",
            "Train Results: MSE: 0.0016, SSIM: 0.9258\n",
            "Model loaded from best_predrnn_model.pth\n",
            "Validation Results: MSE: 0.0020, SSIM: 0.9201\n",
            "Test Results: MSE: 0.0024, SSIM: 0.9065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "target_class = \"Archery\"\n",
        "class_idx = label_encoder.transform([target_class])[0]\n",
        "filtered_df = eval_df[eval_df[\"label_encoded\"] == class_idx]\n",
        "\n",
        "# Extract paths and labels for this class\n",
        "filtered_paths = filtered_df[\"clip_path\"].values\n",
        "filtered_labels = filtered_df[\"label_encoded\"].values\n"
      ],
      "metadata": {
        "id": "em9I1dfAPhTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_dataset = VideoDataset(filtered_paths, filtered_labels, transform=transform)\n",
        "filtered_loader = DataLoader(filtered_dataset, batch_size=4, shuffle=False)\n"
      ],
      "metadata": {
        "id": "CWrtUZd0PhTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "# Function to generate a video from frames\n",
        "def generate_video(frames, save_path=\"generated_video.mp4\", fps=10):\n",
        "    # Check input frames\n",
        "    if len(frames) == 0:\n",
        "        print(\"Error: No frames to write!\")\n",
        "        return\n",
        "    if frames.max() <= 1.0:\n",
        "        frames = np.clip((frames * 255), 0, 255).astype(np.uint8)  # Scale to [0, 255]\n",
        "\n",
        "    # Ensure frames are in [T, H, W, C] format\n",
        "    print(f\"Frames to write: {len(frames)}, Resolution: {frames[0].shape[:2]}\")\n",
        "\n",
        "    height, width = frames[0].shape[:2]\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    video_writer = cv2.VideoWriter(save_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Write frames to video\n",
        "    for i, frame in enumerate(frames):\n",
        "        print(f\"Writing frame {i+1}/{len(frames)}\")\n",
        "        video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))  # OpenCV expects BGR\n",
        "\n",
        "    video_writer.release()\n",
        "    print(f\"Video saved to {save_path}\")\n",
        "\n",
        "\n",
        "# Load trained model\n",
        "model.load_state_dict(torch.load(\"/content/best_predrnn_model.pth\"))  # Path to your trained model\n",
        "model.eval()\n",
        "\n",
        "# Filtered loader for a specific class (this should be defined somewhere)\n",
        "for batch_idx, (input_frames, target_frames) in enumerate(filtered_loader):\n",
        "    input_frames = input_frames.to(device)  # Move input frames to the device\n",
        "\n",
        "    # Predict frames using the model\n",
        "    with torch.no_grad():\n",
        "        pred_frames_batch = model(input_frames).cpu().numpy()  # Predicted frames of shape [batch_size, T, C, H, W]\n",
        "\n",
        "    # Save videos for each batch\n",
        "    for i, pred_frames in enumerate(pred_frames_batch):\n",
        "        # Process the predicted frames\n",
        "        pred_frames = pred_frames.transpose(0, 2, 3, 1)  # Convert to [T, H, W, C]\n",
        "        pred_frames = (pred_frames * 0.5) + 0.5  # Convert to [0, 1]\n",
        "        pred_frames = np.clip(pred_frames, 0, 1)  # Ensure frames are within [0, 1]\n",
        "\n",
        "        # Generate the save path dynamically\n",
        "        save_path = f\"predicted_{target_class}_video_{batch_idx*4 + i + 1}.mp4\"\n",
        "\n",
        "        # Generate the video from predicted frames\n",
        "        generate_video(pred_frames, save_path=save_path, fps=10)\n",
        "        print(f\"Saved: {save_path}\")\n"
      ],
      "metadata": {
        "id": "JXnoHFVLB0b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dea74b6c-7f0a-41de-8068-1ddf5b699645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-76b5e18b5685>:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/best_predrnn_model.pth\"))  # Path to your trained model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_1.mp4\n",
            "Saved: predicted_Archery_video_1.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_2.mp4\n",
            "Saved: predicted_Archery_video_2.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_3.mp4\n",
            "Saved: predicted_Archery_video_3.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_4.mp4\n",
            "Saved: predicted_Archery_video_4.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_5.mp4\n",
            "Saved: predicted_Archery_video_5.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_6.mp4\n",
            "Saved: predicted_Archery_video_6.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_7.mp4\n",
            "Saved: predicted_Archery_video_7.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_8.mp4\n",
            "Saved: predicted_Archery_video_8.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_9.mp4\n",
            "Saved: predicted_Archery_video_9.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_10.mp4\n",
            "Saved: predicted_Archery_video_10.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_11.mp4\n",
            "Saved: predicted_Archery_video_11.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_12.mp4\n",
            "Saved: predicted_Archery_video_12.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_13.mp4\n",
            "Saved: predicted_Archery_video_13.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_14.mp4\n",
            "Saved: predicted_Archery_video_14.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_15.mp4\n",
            "Saved: predicted_Archery_video_15.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_16.mp4\n",
            "Saved: predicted_Archery_video_16.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_17.mp4\n",
            "Saved: predicted_Archery_video_17.mp4\n",
            "Frames to write: 5, Resolution: (64, 64)\n",
            "Writing frame 1/5\n",
            "Writing frame 2/5\n",
            "Writing frame 3/5\n",
            "Writing frame 4/5\n",
            "Writing frame 5/5\n",
            "Video saved to predicted_Archery_video_18.mp4\n",
            "Saved: predicted_Archery_video_18.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Mount Google Drive to access it from Colab\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Upload files from local system\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# Define the folder name (new folder you want to create)\n",
        "folder_name = 'dl_project_data'\n",
        "destination_folder = f'/content/drive/MyDrive/{folder_name}/'\n",
        "\n",
        "# Create the new folder if it doesn't exist\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# Move uploaded files to the new folder\n",
        "for filename in uploaded_files.keys():\n",
        "    shutil.move(filename, destination_folder + filename)\n",
        "    print(f\"Moved {filename} to {destination_folder}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "B-EG-wMGBTnr",
        "outputId": "8f9b78ed-55ec-4275-9f63-93f8d2908fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1ad8afeb-15fe-4b38-828d-875b0a496afe\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1ad8afeb-15fe-4b38-828d-875b0a496afe\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Test.csv to Test (1).csv\n",
            "Saving Train.csv to Train (1).csv\n",
            "Saving Val.csv to Val (1).csv\n",
            "Moved Test (1).csv to /content/drive/MyDrive/dl_project_data/\n",
            "Moved Train (1).csv to /content/drive/MyDrive/dl_project_data/\n",
            "Moved Val (1).csv to /content/drive/MyDrive/dl_project_data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UI - PRED RNN"
      ],
      "metadata": {
        "id": "i29mmZb0fSfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define a file upload button for selecting video\n",
        "def upload_video():\n",
        "    print(\"Please upload your video file.\")\n",
        "    uploaded = files.upload()\n",
        "    video_path = list(uploaded.keys())[0]  # Get the name of the uploaded file\n",
        "    print(f\"Video {video_path} uploaded successfully!\")\n",
        "    return video_path\n",
        "\n",
        "# Preprocessing pipeline for input video frames\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((64, 64)),  # Resize to match model input dimensions\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])  # Normalize to mean 0.5 and std 0.5\n",
        "])\n",
        "\n",
        "# Function to extract frames from video\n",
        "def extract_frames(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(transform(frame).unsqueeze(0))  # Add batch dimension\n",
        "    cap.release()\n",
        "    return torch.cat(frames, dim=0)  # Shape: [num_frames, C, H, W]\n",
        "\n",
        "# Function to save predicted frames as a video\n",
        "def generate_video(frames, save_path=\"predicted_video.mp4\", fps=10):\n",
        "    if len(frames) == 0:\n",
        "        print(\"No frames to save!\")\n",
        "        return\n",
        "    frames = (frames * 255).clip(0, 255).astype(np.uint8)  # Rescale to [0, 255]\n",
        "    height, width = frames[0].shape[:2]\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    video_writer = cv2.VideoWriter(save_path, fourcc, fps, (width, height))\n",
        "    for frame in frames:\n",
        "        video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))  # Convert to BGR\n",
        "    video_writer.release()\n",
        "    print(f\"Video saved to {save_path}\")\n",
        "\n",
        "# Initialize PredRNN model\n",
        "input_frames = 10\n",
        "pred_frames = 5\n",
        "input_dim = 3\n",
        "hidden_dim = 64\n",
        "num_layers = 2\n",
        "height, width = 64, 64\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PredRNN(input_dim, hidden_dim, num_layers, input_frames, pred_frames, height, width).to(device)\n",
        "\n",
        "# Load the trained model state\n",
        "model_path = \"/content/best_predrnn_model.pth\"\n",
        "try:\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    print(f\"Model loaded successfully from {model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Upload video\n",
        "video_path = upload_video()\n",
        "\n",
        "# Process video and predict future frames\n",
        "input_video_frames = extract_frames(video_path)  # Shape: [num_frames, C, H, W]\n",
        "\n",
        "# Ensure enough frames for input\n",
        "if input_video_frames.size(0) < input_frames:\n",
        "    raise ValueError(\"Input video does not contain enough frames for prediction.\")\n",
        "\n",
        "# Prepare input frames batch\n",
        "input_batch = input_video_frames[:input_frames].unsqueeze(0).to(device)  # Shape: [1, T, C, H, W]\n",
        "\n",
        "# Predict future frames\n",
        "with torch.no_grad():\n",
        "    predicted_frames = model(input_batch)  # Shape: [1, pred_frames, C, H, W]\n",
        "    predicted_frames = predicted_frames.squeeze(0).permute(0, 2, 3, 1).cpu().numpy()  # Convert to [T, H, W, C]\n",
        "    predicted_frames = (predicted_frames * 0.5) + 0.5  # Unnormalize to [0, 1]\n",
        "\n",
        "# Save the predicted frames as a video\n",
        "generate_video(predicted_frames, save_path=\"predicted_output.mp4\", fps=10)\n",
        "files.download(\"predicted_output.mp4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "2RvseCUwU66M",
        "outputId": "22a63050-8afc-4473-efbc-22d3cf2f6b9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/best_predrnn_model.pth\n",
            "Please upload your video file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-63-3f4af0e60178>:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-eee847f8-cdc0-4b18-ab0b-bc47f1603e5a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-eee847f8-cdc0-4b18-ab0b-bc47f1603e5a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving v_ApplyLipstick_g01_c05.avi to v_ApplyLipstick_g01_c05 (5).avi\n",
            "Video v_ApplyLipstick_g01_c05 (5).avi uploaded successfully!\n",
            "Video saved to predicted_output.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UI - CONV_LSTM"
      ],
      "metadata": {
        "id": "SA2mUdVefWcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a file upload button for selecting video\n",
        "def upload_video():\n",
        "    print(\"Please upload your video file.\")\n",
        "    uploaded = files.upload()\n",
        "    video_path = list(uploaded.keys())[0]  # Get the name of the uploaded file\n",
        "    print(f\"Video {video_path} uploaded successfully!\")\n",
        "    return video_path\n",
        "\n",
        "# Preprocessing pipeline for input video frames\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((64, 64)),  # Resize to match model input dimensions\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])  # Normalize to mean 0.5 and std 0.5\n",
        "])\n",
        "\n",
        "# Function to extract frames from video\n",
        "def extract_frames(video_path, total_frames):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(transform(frame).unsqueeze(0))  # Add batch dimension\n",
        "    cap.release()\n",
        "\n",
        "    # Ensure the video has enough frames for input + prediction\n",
        "    if len(frames) < total_frames:\n",
        "        frames += [frames[-1]] * (total_frames - len(frames))  # Duplicate last frame if not enough frames\n",
        "    return torch.cat(frames[:total_frames], dim=0)  # Shape: [num_frames, C, H, W]\n",
        "\n",
        "# Function to save predicted frames as a video\n",
        "def generate_video(frames, save_path=\"predicted_video.mp4\", fps=10):\n",
        "    if len(frames) == 0:\n",
        "        print(\"No frames to save!\")\n",
        "        return\n",
        "    frames = (frames * 255).clip(0, 255).astype(np.uint8)  # Rescale to [0, 255]\n",
        "    height, width = frames[0].shape[:2]\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    video_writer = cv2.VideoWriter(save_path, fourcc, fps, (width, height))\n",
        "    for frame in frames:\n",
        "        video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))  # Convert to BGR\n",
        "    video_writer.release()\n",
        "    print(f\"Video saved to {save_path}\")\n",
        "\n",
        "# Define the ConvLSTM model (simplified from the user's original code)\n",
        "class ConvLSTM(nn.Module):\n",
        "    def __init__(self, input_frames, pred_frames, num_classes=1):\n",
        "        super(ConvLSTM, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(3, 32, kernel_size=(3, 3, 3), stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool3d((1, 2, 2))  # Reduces spatial dimensions (H, W) by half\n",
        "        self.lstm = nn.LSTM(input_size=32 * 32 * 32, hidden_size=256, batch_first=True)\n",
        "        self.fc = nn.Linear(256, pred_frames * 32 * 32 * 3)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)  # Restore to 64x64\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Rearrange to match Conv3D input expectations\n",
        "        x = x.permute(0, 2, 1, 3, 4)  # Shape: [batch_size, channels, depth, height, width]\n",
        "\n",
        "        # Convolutional layers\n",
        "        x = self.pool(self.relu(self.conv1(x)))  # Conv3D + Pooling\n",
        "\n",
        "        # Flatten for LSTM\n",
        "        b, c, t, h, w = x.size()\n",
        "        x = x.view(b, t, -1)  # Shape: [batch_size, depth, features]\n",
        "\n",
        "        # LSTM\n",
        "        x, _ = self.lstm(x)  # Shape: [batch_size, depth, hidden_size]\n",
        "        x = self.fc(x[:, -1, :])  # Use the last LSTM output\n",
        "\n",
        "        # Reshape and upsample\n",
        "        x = x.view(b, -1, 3, 32, 32)  # Shape: [batch_size, pred_frames, channels, 32, 32]\n",
        "        x = x.view(-1, 3, 32, 32)  # Flatten pred_frames for upsampling\n",
        "        x = self.upsample(x)  # Upsample to [batch_size * pred_frames, 3, 64, 64]\n",
        "        x = x.view(b, -1, 3, 64, 64)  # Restore batch and pred_frames structure\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "input_frames = 10\n",
        "pred_frames = 5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ConvLSTM(input_frames, pred_frames).to(device)\n",
        "\n",
        "# Load the trained model state\n",
        "model_path = \"/content/best_model.pth\"  # Replace with your actual model path\n",
        "try:\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    print(f\"Model loaded successfully from {model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Upload video\n",
        "video_path = upload_video()\n",
        "\n",
        "# Process video and predict future frames\n",
        "input_video_frames = extract_frames(video_path, total_frames=input_frames + pred_frames)  # Shape: [num_frames, C, H, W]\n",
        "\n",
        "# Prepare input frames batch\n",
        "input_batch = input_video_frames[:input_frames].unsqueeze(0).to(device)  # Shape: [1, T, C, H, W]\n",
        "\n",
        "# Predict future frames\n",
        "with torch.no_grad():\n",
        "    predicted_frames = model(input_batch)  # Shape: [1, pred_frames, C, H, W]\n",
        "    predicted_frames = predicted_frames.squeeze(0).permute(0, 2, 3, 1).cpu().numpy()  # Convert to [T, H, W, C]\n",
        "    predicted_frames = (predicted_frames * 0.5) + 0.5  # Unnormalize to [0, 1]\n",
        "\n",
        "# Save the predicted frames as a video\n",
        "generate_video(predicted_frames, save_path=\"predicted_output.mp4\", fps=10)\n",
        "\n",
        "# Optionally, download the predicted video file\n",
        "files.download(\"predicted_output.mp4\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "6IUd54O2eTzi",
        "outputId": "9bd87361-09d2-41f0-b213-2b8ae67df848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/best_model.pth\n",
            "Please upload your video file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-65-1ec6462a1d4b>:101: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-90dfe9e9-d18a-47a4-8c4d-75af8bcfd7fc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-90dfe9e9-d18a-47a4-8c4d-75af8bcfd7fc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving v_ApplyLipstick_g01_c05.avi to v_ApplyLipstick_g01_c05 (6).avi\n",
            "Video v_ApplyLipstick_g01_c05 (6).avi uploaded successfully!\n",
            "Video saved to predicted_output.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a7bfaa75-7c31-43af-b087-487a662f7005\", \"predicted_output.mp4\", 2248)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}